# -*- coding: utf-8 -*-
"""Vectorización_TF-IDF_conteo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-oYvk9gWROSSwjwCojSpVL9sz38-TALy
"""

# Subir el archivo CSV desde tu computadora
from google.colab import files
uploaded = files.upload()

# Importar pandas y leer el archivo CSV
import pandas as pd

# Obtén el nombre del archivo subido
file_name = list(uploaded.keys())[0]

# Lee el archivo CSV en un DataFrame de pandas
df = pd.read_csv(file_name)

# Muestra las primeras filas del DataFrame
df.head()

# Cargamos las librerias necesarias
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import nltk
from nltk import pos_tag, word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
import string

# Analizamos los tipos de datos
df.dtypes

# Seleccionar un subconjunto de columnas
selected_columns = ['Customer_Name', 'Product', 'Category', 'Brand']
filtered_data = df[selected_columns]

# Mostrar las primeras filas del dataset filtrado
print("Dataset Filtrado:")
print(filtered_data.head())

#Limpieza de datos

# Verificar valores nulos
null_values = filtered_data.isnull().sum()
print(null_values)

# Eliminar filas con valores nulos
data_cleaned = filtered_data.dropna()

# Verificar que no quedan valores nulos
print(data_cleaned.isnull().sum())

import pandas as pd
# Función para combinar las columnas en una sola cadena de texto
def combine_columns(row):
    return f"{row['Product']} {row['Category']} {row['Brand']}"

# Aplicar la función a lo largo de las filas del DataFrame para crear la nueva columna
data_cleaned['Combined_Text'] = data_cleaned.apply(lambda row: combine_columns(row), axis=1)

# Mostrar el DataFrame resultante
print(data_cleaned)

#Obtenemos las palabras comunes del idioma inglés
stopwords = nltk.corpus.stopwords.words('english')
stopwords

# Creamos una funcion para procesar el texto
def text_process(text):
    exclude = set(string.punctuation) # Eliminamos los signos de puntuacion
    text = ''.join(ch for ch in text if ch not in exclude)
    tokens = word_tokenize(text.lower()) # Cambiamos a minúsculas y tokenizamos
    #tokens_stripped = [ts.strip("""'".,;:-():!?-‘’#$%&_-1234567890*+/][¨`´ç·<>}{”- """) for ts in tokens] # Eliminamos los signos de puntuacion
    tokens_stripped = [ts.strip('“0123456789” ') for ts in tokens] # Eliminamos otros elementos
    nopunc = ' '.join(tokens_stripped) # Agrupamos todos los tokens en un texto separado por espacios
    return [word for word in nopunc.split() if word not in stopwords] # ELiminasmos las stopwords

"""VECTORIZADOR TF-IDF"""

# Creamos el vectorizador
tfidfvectorizer = TfidfVectorizer(analyzer = text_process, norm = None)

# Calculamos la matrix de documentos y términos
tfidf= tfidfvectorizer.fit_transform(data_cleaned['Combined_Text'])

tfidf_tokens = tfidfvectorizer.get_feature_names_out()
df_tfidfvect = pd.DataFrame(data = tfidf.toarray(),index = data_cleaned['Customer_Name'],columns = tfidf_tokens)
print("\nTD-IDF Vectorizer\n")
print(df_tfidfvect)

df_tfidfvect["msi"].sort_values(ascending=False).head(20)

ProductosEscalares = df_tfidfvect.dot(df_tfidfvect.T)
ProductosEscalares

Normas = np.diag(ProductosEscalares)**(1/2)
Normas

Similaridades = ((1/Normas)*(ProductosEscalares)).T*(1/Normas)
Similaridades

"""Vectorización por conteo"""

# Seleccionar un subconjunto de columnas
selected_columns = ['Customer_Name', 'Product', 'Category' , 'Brand', 'Quantity']
filtered_data = df[selected_columns]

# Mostrar las primeras filas del dataset filtrado
print("Dataset Filtrado:")
print(filtered_data.head())

#Limpieza de datos

# Verificar valores nulos
null_values = filtered_data.isnull().sum()
print(null_values)

# Eliminar filas con valores nulos
data_cleaned = filtered_data.dropna()

# Verificar que no quedan valores nulos
print(data_cleaned.isnull().sum())

import pandas as pd
# Función para combinar las columnas en una sola cadena de texto
def combine_columns(row):
    return f"{row['Product']} {row['Category']} {row['Brand']}"

# Aplicar la función a lo largo de las filas del DataFrame para crear la nueva columna
data_cleaned['Combined_Text'] = data_cleaned.apply(lambda row: combine_columns(row), axis=1)

# Mostrar el DataFrame resultante
print(data_cleaned)

#Obtenemos las palabras comunes del idioma inglés
stopwords = nltk.corpus.stopwords.words('english')
stopwords

# Creamos una funcion para procesar el texto
def text_process(text):
    exclude = set(string.punctuation) # Eliminamos los signos de puntuacion
    text = ''.join(ch for ch in text if ch not in exclude)
    tokens = word_tokenize(text.lower()) # Cambiamos a minúsculas y tokenizamos
    #tokens_stripped = [ts.strip("""'".,;:-():!?-‘’#$%&_-1234567890*+/][¨`´ç·<>}{”- """) for ts in tokens] # Eliminamos los signos de puntuacion
    tokens_stripped = [ts.strip('“0123456789” ') for ts in tokens] # Eliminamos otros elementos
    nopunc = ' '.join(tokens_stripped) # Agrupamos todos los tokens en un texto separado por espacios
    return [word for word in nopunc.split() if word not in stopwords] # ELiminasmos las stopwords

# Creamos el vectorizador
countvectorizer = CountVectorizer(analyzer= text_process)

# Calculamos la matrix de documentos y términos
count = countvectorizer.fit_transform(data_cleaned['Combined_Text'])

count_tokens = countvectorizer.get_feature_names_out()
df_countvect = pd.DataFrame(data = count.toarray(),index = data_cleaned['Customer_Name'],columns = count_tokens)
print("\nCOUNT Vectorizer\n")
print(df_countvect)